{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting undetected-chromedriver\n",
      "  Downloading undetected-chromedriver-3.5.5.tar.gz (65 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from undetected-chromedriver) (4.28.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from undetected-chromedriver) (2.32.3)\n",
      "Collecting websockets (from undetected-chromedriver)\n",
      "  Downloading websockets-15.0-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->undetected-chromedriver) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->undetected-chromedriver) (3.7)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yujit\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.14.0)\n",
      "Downloading websockets-15.0-cp311-cp311-win_amd64.whl (176 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (setup.py): started\n",
      "  Building wheel for undetected-chromedriver (setup.py): finished with status 'done'\n",
      "  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.5-py3-none-any.whl size=47130 sha256=d8753ab4505c39033225b01bd63570ca645c8e281acfbbb2013c73e5057b6b2e\n",
      "  Stored in directory: c:\\users\\yujit\\appdata\\local\\pip\\cache\\wheels\\5c\\b9\\03\\4b6e38f019d6170e8c25df2e1e362d7bdf9ff4012df2dc85c0\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: websockets, undetected-chromedriver\n",
      "Successfully installed undetected-chromedriver-3.5.5 websockets-15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install undetected-chromedriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 15 articles...\n",
      "Scraped 40 articles...\n",
      "Scraped 70 articles...\n",
      "Scraped 110 articles...\n",
      "Scraped 150 articles...\n",
      "Scraped 190 articles...\n",
      "Scraped 234 articles...\n",
      "Scraped 288 articles...\n",
      "Scraped 349 articles...\n",
      "Scraped 420 articles...\n",
      "Scraped 500 articles...\n",
      "Scraped 582 articles...\n",
      "Scraped 674 articles...\n",
      "Scraped 776 articles...\n",
      "Scraped 885 articles...\n",
      "Scraped 1003 articles...\n",
      "Scraped 1123 articles...\n",
      "Scraped 1250 articles...\n",
      "Scraped 1387 articles...\n",
      "Scraped 1524 articles...\n",
      "Scraped 1661 articles...\n",
      "Scraped 1798 articles...\n",
      "Scraped 1935 articles...\n",
      "Scraped 2072 articles...\n",
      "Scraped 2209 articles...\n",
      "Scraped 2346 articles...\n",
      "Scraped 2483 articles...\n",
      "Scraped 2620 articles...\n",
      "Scraped 2757 articles...\n",
      "Scraped 2894 articles...\n",
      "Scraped 3031 articles...\n",
      "Scraped 3168 articles...\n",
      "Scraped 3305 articles...\n",
      "Scraped 3442 articles...\n",
      "Scraped 3579 articles...\n",
      "Scraped 3716 articles...\n",
      "Scraped 3853 articles...\n",
      "Scraped 3990 articles...\n",
      "Scraped 4127 articles...\n",
      "Scraped 4264 articles...\n",
      "Scraped 4401 articles...\n",
      "Scraped 4538 articles...\n",
      "Scraped 4675 articles...\n",
      "Scraped 4812 articles...\n",
      "Scraped 4949 articles...\n",
      "Scraped 5086 articles...\n",
      "Scraped 5223 articles...\n",
      "Scraped 5360 articles...\n",
      "Scraped 5497 articles...\n",
      "Scraped 5634 articles...\n",
      "Scraped 5771 articles...\n",
      "Scraped 5908 articles...\n",
      "Scraped 6045 articles...\n",
      "Scraped 6182 articles...\n",
      "Scraped 6319 articles...\n",
      "Scraped 6456 articles...\n",
      "Scraped 6593 articles...\n",
      "Scraped 6730 articles...\n",
      "Scraped 6867 articles...\n",
      "Scraped 7004 articles...\n",
      "Scraped 7141 articles...\n",
      "Scraped 7278 articles...\n",
      "Scraped 7415 articles...\n",
      "Scraped 7552 articles...\n",
      "Scraped 7689 articles...\n",
      "Scraped 7826 articles...\n",
      "Scraped 7963 articles...\n",
      "Scraped 8100 articles...\n",
      "Scraped 8237 articles...\n",
      "Scraped 8374 articles...\n",
      "Scraped 8511 articles...\n",
      "Scraped 8648 articles...\n",
      "Scraped 8785 articles...\n",
      "Scraped 8922 articles...\n",
      "Data saved to inshorts_news.xlsx with 8922 articles.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# Setup WebDriver\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')  # Bypass bot detection\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "    \n",
    "    # Specify the correct path for your ChromeDriver\n",
    "    service = Service('C:\\\\Users\\\\yujit\\\\OneDrive\\\\Desktop\\\\chromedriver-win64\\\\chromedriver.exe')\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "# Function to scrape news articles from Inshorts\n",
    "def scrape_news(url=\"https://inshorts.com/en/read\"):\n",
    "    driver = setup_driver()\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Let page load\n",
    "\n",
    "    news_data = []\n",
    "    start_time = time.time()  # Start timer\n",
    "    max_time = 600  # Stop after 10 minutes (600 seconds)\n",
    "\n",
    "    while time.time() - start_time < max_time:\n",
    "        try:\n",
    "            # Wait for news articles to load\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"Hxtmf9GvkV8Ti6V0GUSn\"))\n",
    "            )\n",
    "\n",
    "            articles = driver.find_elements(By.CLASS_NAME, \"Hxtmf9GvkV8Ti6V0GUSn\")  # News body\n",
    "            headlines = driver.find_elements(By.CLASS_NAME, \"S2DdZEgzkqC9bYeTJUGw\")  # Headlines\n",
    "            dates = driver.find_elements(By.CLASS_NAME, \"date\")  # Dates\n",
    "\n",
    "            for i in range(len(articles)):\n",
    "                try:\n",
    "                    title = headlines[i].text.strip()\n",
    "                    content = articles[i].text.strip()\n",
    "                    date = dates[i].text.strip()\n",
    "                    news_data.append([title, content, date])\n",
    "                except IndexError:\n",
    "                    continue  # Skip if any element is missing\n",
    "            \n",
    "            print(f\"Scraped {len(news_data)} articles...\")\n",
    "\n",
    "            # Click 'Load More' button to load more articles\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, \"QMXJlc3R5MMJjDGSV4Jd\"))\n",
    "                )\n",
    "                load_more_button.click()\n",
    "                time.sleep(random.uniform(3, 6))  # Random delay to mimic human behavior\n",
    "            except TimeoutException:\n",
    "                print(\"No more 'Load More' button found.\")\n",
    "                break  # Stop if button is not found\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page elements to load.\")\n",
    "            break  # Stop scraping if elements are not found\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save data to Excel\n",
    "    if news_data:\n",
    "        df = pd.DataFrame(news_data, columns=[\"Headline\", \"News\", \"Date\"])\n",
    "        df.to_excel(\"inshorts_news.xlsx\", index=False)\n",
    "        print(f\"Data saved to inshorts_news.xlsx with {len(news_data)} articles.\")\n",
    "    else:\n",
    "        print(\"No news articles were scraped.\")\n",
    "\n",
    "# Run the scraper\n",
    "scrape_news()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
