{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b85c251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (4.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (3.1.5)\n",
      "Requirement already satisfied: urllib3~=1.26 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from urllib3[secure]~=1.26->selenium) (1.26.20)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from urllib3[secure]~=1.26->selenium) (25.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from urllib3[secure]~=1.26->selenium) (44.0.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from urllib3[secure]~=1.26->selenium) (2025.1.31)\n",
      "Requirement already satisfied: urllib3-secure-extra in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from urllib3[secure]~=1.26->selenium) (0.1.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: typing-extensions>=4.9 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (4.12.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yujit\\appdata\\roaming\\python\\python311\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5f3802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Product Scraper (Visible Browser Mode)\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 18:34:10,715 - INFO - Scraping page: https://www.amazon.in/s?k=clothes&crid=1VSYJVZO6HQIZ&sprefix=clothes%2Caps%2C266&ref=nb_sb_noss_2\n",
      "2025-04-11 18:34:35,148 - INFO - Found 130 products on page\n",
      "2025-04-11 18:34:35,719 - INFO - Scraped 1 pages, 130 products collected\n",
      "2025-04-11 18:34:35,721 - INFO - Scraping page: https://www.amazon.in/s?k=clothes&page=2&xpid=pyDoFj3naw10q&crid=1VSYJVZO6HQIZ&qid=1744376652&sprefix=clothes%2Caps%2C266&ref=sr_pg_2\n",
      "2025-04-11 18:35:09,931 - INFO - Found 130 products on page\n",
      "2025-04-11 18:35:10,473 - INFO - Scraped 2 pages, 260 products collected\n",
      "2025-04-11 18:35:10,475 - INFO - Scraping page: https://www.amazon.in/s?k=clothes&page=3&xpid=pyDoFj3naw10q&crid=1VSYJVZO6HQIZ&qid=1744376652&sprefix=clothes%2Caps%2C266&ref=sr_pg_3\n",
      "2025-04-11 18:35:47,570 - INFO - Found 130 products on page\n",
      "2025-04-11 18:35:48,058 - INFO - Scraped 3 pages, 390 products collected\n",
      "2025-04-11 18:35:48,059 - INFO - Scraping page: https://www.amazon.in/s?k=clothes&crid=1VSYJVZO6HQIZ&qid=1744376678&sprefix=clothes%2Caps%2C266&xpid=pyDoFj3naw10q&ref=sr_pg_1\n",
      "2025-04-11 18:35:54,633 - INFO - Found 130 products on page\n",
      "2025-04-11 18:35:55,260 - INFO - Scraped 4 pages, 520 products collected\n",
      "2025-04-11 18:35:55,261 - INFO - Scraping page: https://www.amazon.in/s?k=clothes&page=3&crid=1VSYJVZO6HQIZ&qid=1744376678&sprefix=clothes%2Caps%2C266&xpid=pyDoFj3naw10q&ref=sr_pg_3\n",
      "2025-04-11 18:36:01,618 - INFO - Found 130 products on page\n",
      "2025-04-11 18:36:02,087 - INFO - Scraped 5 pages, 650 products collected\n",
      "2025-04-11 18:36:02,088 - INFO - Scraping page: https://www.amazon.in/s?k=clothes&crid=1VSYJVZO6HQIZ&qid=1744376713&sprefix=clothes%2Caps%2C266&xpid=pyDoFj3naw10q&ref=sr_pg_1\n",
      "2025-04-11 18:36:08,630 - INFO - Found 130 products on page\n",
      "2025-04-11 18:36:09,047 - INFO - Scraped 6 pages, 780 products collected\n",
      "2025-04-11 18:36:09,048 - INFO - Scraping page: https://www.amazon.in/s?k=clothes&page=2&crid=1VSYJVZO6HQIZ&qid=1744376713&sprefix=clothes%2Caps%2C266&xpid=pyDoFj3naw10q&ref=sr_pg_2\n",
      "2025-04-11 18:36:14,235 - INFO - Found 130 products on page\n",
      "2025-04-11 18:36:14,645 - INFO - Scraped 7 pages, 910 products collected\n",
      "2025-04-11 18:36:14,657 - INFO - Saved 910 products to amo.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completed successfully!\n",
      "Scraped 910 products\n",
      "Results saved to amo.csv\n",
      "\n",
      "Sample products:\n",
      "\n",
      "- Men's Wrinkle-Resistant Regular Fit Cotton Formal Shirt\n",
      "  Price: ₹1899 | Rating: 4.0 stars\n",
      "  Reviews: 549 | Delivery: Sun, 13 Apr\n",
      "  URL: https://www.amazon.in/sspa/click...\n",
      "\n",
      "- Men's Wrinkle-Resistant Regular Fit Cotton Formal Shirt\n",
      "  Price: ₹1899 | Rating: 4.0 stars\n",
      "  Reviews: 549 | Delivery: Sun, 13 Apr\n",
      "  URL: https://www.amazon.in/sspa/click...\n",
      "\n",
      "- Boldfit for Men Slim Fit Joggers for Men for Running, Gym Sports Lower for Men & Boys Summer Track Pants for Men Multipurpose Mens Lower Activewear Trousers, Night Pants for Men Black M\n",
      "  Price: ₹699 | Rating: 4.2 stars\n",
      "  Reviews: 1,118 | Delivery: Sun, 13 Apr\n",
      "  URL: https://www.amazon.in/sspa/click...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 275\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    273\u001b[39m     \u001b[38;5;66;03m# Close browser\u001b[39;00m\n\u001b[32m    274\u001b[39m     scraper.close()\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mPress Enter to exit...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Keep window open to observe any errors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "import csv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Product:\n",
    "    title: str\n",
    "    price: str\n",
    "    rating: Optional[str]\n",
    "    reviews: Optional[str]\n",
    "    delivery: Optional[str]\n",
    "    url: str\n",
    "    sponsored: bool = False\n",
    "    deal: Optional[str] = None\n",
    "\n",
    "class AmazonScraper:\n",
    "    def __init__(self, visible_browser=True):\n",
    "        self.base_url = \"https://www.amazon.in\"\n",
    "        self.visible_browser = visible_browser\n",
    "        self.driver = self._init_driver()\n",
    "        self.delay_range = (1, 3)  # Random delay between actions in seconds\n",
    "\n",
    "    def _init_driver(self):\n",
    "        \"\"\"Initialize Chrome driver with visible browser\"\"\"\n",
    "        options = Options()\n",
    "        if not self.visible_browser:\n",
    "            options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        return driver\n",
    "\n",
    "    def _random_delay(self):\n",
    "        \"\"\"Add random delay between actions\"\"\"\n",
    "        delay = random.uniform(*self.delay_range)\n",
    "        time.sleep(delay)\n",
    "\n",
    "    def _validate_amazon_url(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL is valid Amazon URL\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc.endswith('amazon.in') or parsed.netloc.endswith('amazon.com')\n",
    "\n",
    "    def _get_page(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Load page and return its HTML\"\"\"\n",
    "        try:\n",
    "            self._random_delay()\n",
    "            self.driver.get(url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.s-result-item\")))\n",
    "            \n",
    "            # Check for CAPTCHA\n",
    "            if \"api-services-support@amazon.com\" in self.driver.page_source:\n",
    "                raise Exception(\"CAPTCHA detected - Amazon is blocking requests\")\n",
    "                \n",
    "            return self.driver.page_source\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading page {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_product_data(self, item) -> Optional[Product]:\n",
    "        \"\"\"Extract product data from BeautifulSoup element\"\"\"\n",
    "        try:\n",
    "            # Extract product URL\n",
    "            link = item.select_one('h2 a.a-link-normal, a.a-link-normal.s-line-clamp-2, a.a-link-normal.s-line-clamp-3')\n",
    "            if not link:\n",
    "                return None\n",
    "                \n",
    "            raw_url = link.get('href', '')\n",
    "            product_url = urljoin(self.base_url, raw_url.split('?')[0])\n",
    "\n",
    "            # Extract title\n",
    "            title = link.get_text(strip=True)\n",
    "\n",
    "            # Extract price\n",
    "            price_whole = item.select_one('span.a-price-whole')\n",
    "            price = price_whole.get_text(strip=True).replace(',', '') if price_whole else None\n",
    "\n",
    "            # Extract rating\n",
    "            rating = item.select_one('span.a-icon-alt')\n",
    "            rating = rating.get_text(strip=True).split()[0] if rating else None\n",
    "\n",
    "            # Extract reviews\n",
    "            reviews = item.select_one('span.a-size-base[aria-label], span.a-size-base.s-underline-text')\n",
    "            reviews = reviews.get_text(strip=True) if reviews else None\n",
    "\n",
    "            # Extract delivery\n",
    "            delivery = item.select_one('span.a-color-base.a-text-bold')\n",
    "            delivery = delivery.get_text(strip=True) if delivery else None\n",
    "\n",
    "            # Extract deal\n",
    "            deal = item.select_one('span.a-badge-text')\n",
    "            deal = deal.get_text(strip=True) if deal else None\n",
    "\n",
    "            # Check if sponsored\n",
    "            sponsored = bool(item.select_one('span.a-color-secondary:contains(\"Sponsored\"), '\n",
    "                           'span:contains(\"Sponsored Ad\")'))\n",
    "\n",
    "            return Product(\n",
    "                title=title,\n",
    "                price=price,\n",
    "                rating=rating,\n",
    "                reviews=reviews,\n",
    "                delivery=delivery,\n",
    "                url=product_url,\n",
    "                sponsored=sponsored,\n",
    "                deal=deal\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting product data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_pagination_links(self, soup) -> List[str]:\n",
    "        \"\"\"Extract pagination links from page\"\"\"\n",
    "        pagination_div = soup.select_one('div.s-pagination-strip, div.a-text-center.s-pagination-container')\n",
    "        if not pagination_div:\n",
    "            return []\n",
    "\n",
    "        page_links = []\n",
    "        for a in pagination_div.select('a.s-pagination-item'):\n",
    "            if a.text.strip().isdigit():\n",
    "                page_url = urljoin(self.base_url, a['href'])\n",
    "                if page_url not in page_links:\n",
    "                    page_links.append(page_url)\n",
    "        return page_links\n",
    "\n",
    "    def scrape_search_page(self, url: str) -> Tuple[List[Product], List[str]]:\n",
    "        \"\"\"Scrape a single search page\"\"\"\n",
    "        if not self._validate_amazon_url(url):\n",
    "            logger.error(\"Invalid Amazon URL provided\")\n",
    "            return [], []\n",
    "\n",
    "        logger.info(f\"Scraping page: {url}\")\n",
    "        html = self._get_page(url)\n",
    "        if not html:\n",
    "            return [], []\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        products = []\n",
    "        \n",
    "        items = soup.select('div.s-result-item[data-component-type=\"s-search-result\"], '\n",
    "                           'div.a-section.a-spacing-small.puis-padding-left-small, '\n",
    "                           'div.a-section.a-spacing-small.puis-padding-left-micro')\n",
    "        logger.info(f\"Found {len(items)} products on page\")\n",
    "\n",
    "        for item in items:\n",
    "            product = self._extract_product_data(item)\n",
    "            if product:\n",
    "                products.append(product)\n",
    "\n",
    "        next_pages = self._get_pagination_links(soup)\n",
    "        return products, next_pages\n",
    "\n",
    "    def scrape_search(self, search_url: str, max_pages: Optional[int] = None) -> List[Product]:\n",
    "        \"\"\"Scrape multiple pages of search results\"\"\"\n",
    "        if not self._validate_amazon_url(search_url):\n",
    "            logger.error(\"Invalid Amazon URL provided\")\n",
    "            return []\n",
    "\n",
    "        all_products = []\n",
    "        pages_to_scrape = [search_url]\n",
    "        pages_scraped = 0\n",
    "        scraped_urls = set()\n",
    "\n",
    "        while pages_to_scrape and (max_pages is None or pages_scraped < max_pages):\n",
    "            current_url = pages_to_scrape.pop(0)\n",
    "            if current_url in scraped_urls:\n",
    "                continue\n",
    "                \n",
    "            products, new_pages = self.scrape_search_page(current_url)\n",
    "            \n",
    "            all_products.extend(products)\n",
    "            pages_scraped += 1\n",
    "            scraped_urls.add(current_url)\n",
    "            \n",
    "            for page in new_pages:\n",
    "                if page not in scraped_urls and page not in pages_to_scrape:\n",
    "                    pages_to_scrape.append(page)\n",
    "\n",
    "            logger.info(f\"Scraped {pages_scraped} pages, {len(all_products)} products collected\")\n",
    "            \n",
    "            if max_pages and pages_scraped >= max_pages:\n",
    "                break\n",
    "\n",
    "        return all_products\n",
    "\n",
    "    def save_to_csv(self, products: List[Product], filename: str):\n",
    "        \"\"\"Save products to CSV\"\"\"\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['title', 'price', 'rating', 'reviews', \n",
    "                                                 'delivery', 'url', 'sponsored', 'deal'])\n",
    "            writer.writeheader()\n",
    "            for product in products:\n",
    "                writer.writerow(product.__dict__)\n",
    "        logger.info(f\"Saved {len(products)} products to {filename}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the browser\"\"\"\n",
    "        self.driver.quit()\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get user input for scraping parameters\"\"\"\n",
    "    print(\"Amazon Product Scraper (Visible Browser Mode)\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    while True:\n",
    "        url = input(\"Enter Amazon search URL (e.g., https://www.amazon.in/s?k=mobiles): \").strip()\n",
    "        if not url:\n",
    "            print(\"URL cannot be empty. Please try again.\")\n",
    "            continue\n",
    "            \n",
    "        max_pages = input(\"How many pages to scrape? (Press Enter for all pages): \").strip()\n",
    "        try:\n",
    "            max_pages = int(max_pages) if max_pages else None\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number or press Enter for all pages.\")\n",
    "            continue\n",
    "            \n",
    "        filename = input(\"Enter output CSV filename (default: amazon_products.csv): \").strip()\n",
    "        filename = filename or \"amazon_products.csv\"\n",
    "        \n",
    "        return url, max_pages, filename\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get user input\n",
    "    search_url, max_pages, filename = get_user_input()\n",
    "    \n",
    "    # Initialize scraper with visible browser\n",
    "    scraper = AmazonScraper(visible_browser=True)\n",
    "    \n",
    "    try:\n",
    "        # Run scraping\n",
    "        products = scraper.scrape_search(search_url, max_pages)\n",
    "        \n",
    "        # Save results\n",
    "        scraper.save_to_csv(products, filename)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nScraping completed successfully!\")\n",
    "        print(f\"Scraped {len(products)} products\")\n",
    "        print(f\"Results saved to {filename}\")\n",
    "        \n",
    "        # Print sample\n",
    "        print(\"\\nSample products:\")\n",
    "        for product in products[:3]:\n",
    "            print(f\"\\n- {product.title}\")\n",
    "            print(f\"  Price: ₹{product.price} | Rating: {product.rating or 'N/A'} stars\")\n",
    "            print(f\"  Reviews: {product.reviews or 'N/A'} | Delivery: {product.delivery or 'N/A'}\")\n",
    "            print(f\"  URL: {product.url[:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred during scraping: {e}\")\n",
    "    finally:\n",
    "        # Close browser\n",
    "        scraper.close()\n",
    "        input(\"\\nPress Enter to exit...\")  # Keep window open to observe any errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
