{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from Page 1. Clicking Next until Page 1200...\n",
      "Reached Page 2\n",
      "Reached Page 3\n",
      "Reached Page 4\n",
      "Reached Page 5\n",
      "Reached Page 6\n",
      "Reached Page 7\n",
      "Reached Page 8\n",
      "Reached Page 9\n",
      "Reached Page 10\n",
      "Reached Page 11\n",
      "Reached Page 12\n",
      "Reached Page 13\n",
      "Reached Page 14\n",
      "Reached Page 15\n",
      "Reached Page 16\n",
      "Reached Page 17\n",
      "Reached Page 18\n",
      "Reached Page 19\n",
      "Reached Page 20\n",
      "Reached Page 21\n",
      "Reached Page 22\n",
      "Reached Page 23\n",
      "Reached Page 24\n",
      "Reached Page 25\n",
      "Reached Page 26\n",
      "Reached Page 27\n",
      "Reached Page 28\n",
      "Reached Page 29\n",
      "Reached Page 30\n",
      "Reached Page 31\n",
      "Reached Page 32\n",
      "Reached Page 33\n",
      "Reached Page 34\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Selenium setup\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "chrome_options.add_argument(\"--log-level=3\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "\n",
    "driver_path = \"C:\\\\Users\\\\yujit\\\\OneDrive\\\\Desktop\\\\chromedriver-win64\\\\chromedriver.exe\"  # Change this to your ChromeDriver path\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Folder to store data\n",
    "SAVE_FOLDER = \"Australiadt\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "# Load last scraped page (if exists)\n",
    "resume_file = os.path.join(SAVE_FOLDER, \"resume_page.txt\")\n",
    "start_page = 1\n",
    "if os.path.exists(resume_file):\n",
    "    with open(resume_file, \"r\") as f:\n",
    "        start_page = int(f.read().strip())\n",
    "\n",
    "data_list = []\n",
    "pages_scraped = 0\n",
    "file_counter = (start_page - 1) // 10 + 1  # Start file numbering correctly\n",
    "\n",
    "# Step 1: Start on Page 1 and Click Next Until Last Scraped Page\n",
    "driver.get(\"https://data.gov.au/search?page=1\")\n",
    "print(f\"Starting from Page 1. Clicking Next until Page {start_page}...\")\n",
    "\n",
    "current_page = 1\n",
    "while current_page < start_page:\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.btn-next\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        current_page += 1\n",
    "        time.sleep(random.uniform(3, 5))  # Random delay to mimic human behavior\n",
    "        print(f\"Reached Page {current_page}\")\n",
    "    except:\n",
    "        print(\"Next button not found. Stopping early.\")\n",
    "        break\n",
    "\n",
    "# Step 2: Start Scraping from Last Scraped Page\n",
    "print(f\"Starting scraping from Page {start_page} onwards...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(f\"Scraping Page {start_page}\")\n",
    "\n",
    "        # Wait for datasets to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"li.search-results__result\"))\n",
    "        )\n",
    "\n",
    "        datasets = driver.find_elements(By.CSS_SELECTOR, \"li.search-results__result\")\n",
    "\n",
    "        if not datasets:\n",
    "            print(\"No more data found. Scraping complete!\")\n",
    "            break\n",
    "\n",
    "        for dataset in datasets:\n",
    "            try:\n",
    "                title_element = dataset.find_elements(By.CSS_SELECTOR, \".dataset-summary-title a\")\n",
    "                title = title_element[0].text.strip() if title_element else \"N/A\"\n",
    "\n",
    "                publisher_element = dataset.find_elements(By.CSS_SELECTOR, \".dataset-summary-publisher\")\n",
    "                publisher = publisher_element[0].text.strip() if publisher_element else \"N/A\"\n",
    "\n",
    "                updated_element = dataset.find_elements(By.CSS_SELECTOR, \".dataset-summary-updated\")\n",
    "                updated_date = updated_element[0].text.replace(\"Dataset Updated \", \"\").strip() if updated_element else \"N/A\"\n",
    "\n",
    "                # Extract available types\n",
    "                types_elements = dataset.find_elements(By.CSS_SELECTOR, \".dataset-summary-downloads span\")\n",
    "                types = \", \".join([type_el.text.strip() for type_el in types_elements if type_el.text.strip()]) if types_elements else \"N/A\"\n",
    "\n",
    "                data_list.append([title, publisher, updated_date, types])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting dataset: {e}\")\n",
    "\n",
    "        pages_scraped += 1\n",
    "        start_page += 1\n",
    "\n",
    "        # Save data every 10 pages\n",
    "        if pages_scraped % 10 == 0:\n",
    "            df = pd.DataFrame(data_list, columns=[\"Title\", \"Publisher\", \"Updated Date\", \"Types Available\"])\n",
    "            file_path = os.path.join(SAVE_FOLDER, f\"Australian_Data_{file_counter}.xlsx\")\n",
    "            df.to_excel(file_path, index=False)\n",
    "            print(f\"Saved: {file_path}\")\n",
    "\n",
    "            data_list = []\n",
    "            file_counter += 1\n",
    "\n",
    "            with open(resume_file, \"w\") as f:\n",
    "                f.write(str(start_page))\n",
    "\n",
    "        # Click next page\n",
    "        try:\n",
    "            next_page_button = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.btn-next\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", next_page_button)\n",
    "            time.sleep(random.uniform(3, 6))  # Random delay\n",
    "        except:\n",
    "            print(f\"Next page button not found. Stopping at page {start_page}.\")\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {e}\")\n",
    "    with open(resume_file, \"w\") as f:\n",
    "        f.write(str(start_page))\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "    if data_list:\n",
    "        df = pd.DataFrame(data_list, columns=[\"Title\", \"Publisher\", \"Updated Date\", \"Types Available\"])\n",
    "        file_path = os.path.join(SAVE_FOLDER, f\"Australian_Data_{file_counter}.xlsx\")\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"Saved remaining data: {file_path}\")\n",
    "\n",
    "        with open(resume_file, \"w\") as f:\n",
    "            f.write(str(start_page))\n",
    "\n",
    "    print(\"Scraping finished!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
